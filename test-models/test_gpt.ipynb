{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5caba2",
   "metadata": {},
   "source": [
    "# Replicate Bott & Noveck (2004) with GPT 3 and 4\n",
    "\n",
    "### Compare human's reaction time data with LLMs to see which meaning that LLMs assess first when processing scalar terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbad0dc",
   "metadata": {},
   "source": [
    "Exp1 (zero-shot)\n",
    "\n",
    "\"Evaluate whether this sentence is True or False\"\n",
    "-> decide whether to provide category membership or not.\n",
    "\n",
    "Exp2 (one-shot: semantic/pragmatic instruction) \n",
    "\n",
    "pragmatic prompt: \n",
    "\"The word 'some' can be understood in several ways. One way is to understand it as some but not all. Thus, a sentence like 'Some daffodils are flowers' should be considered false because, in fact, all daffodils are flowers. Now evaluate whether the following sentence is true or false.\"\n",
    "\n",
    "semantic prompt:\n",
    "\"The word 'some' can be understood in several ways. One way is to understand it as some and possibly all. Thus, a sentence like 'Some daffodils are flowers' should be considered true because, even though all daffodils are flowers. Now evaluate whether the following sentence is true or false.\"\n",
    "\n",
    "-> get perplexity (correct answers) and compare surprisal\n",
    "\n",
    "Exp3: Give true category membership and run base/sem/prag instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed355d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04c8df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = open('../openai-keys.txt')\n",
    "lines = keys.readlines()\n",
    "openai.organization = lines[0].rstrip()\n",
    "openai.api_key = lines[1].rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a59a98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt format from Bott & Noveck (2004)\n",
    "\n",
    "#base_prompt\n",
    "#\"Evaluate whether the following sentence is true or false.\n",
    "#prag_prompt\n",
    "#The word 'some' can be understood in several ways. \n",
    "#One way is to understand it as some but not all. \n",
    "#Thus, a sentence like 'Some daffodils are flowers' should be considered false because, \n",
    "#in fact, all daffodils are flowers. \n",
    "#Now evaluate whether the following sentence is true or false. \n",
    "\n",
    "# sem_prompt\n",
    "# The word 'some' can be understood in several ways.\n",
    "# One way is to understand it as some and possibly all.\n",
    "# Thus, a sentence like 'Some daffodils are flowers' should be\n",
    "# considered true, even though we know that all daffodils are flowers.\n",
    "# Now evaluate whether the following sentence is true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633a6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_prompt(sentence):\n",
    "    return (\"Evaluate whether the following sentence \"\n",
    "        \"in the double quotation marks is true or false. \"\n",
    "        \"Respond with only one word, either true or false. \"\n",
    "        \"Do not include any new lines or empty spaces. \"\n",
    "        \"\\\"{sentence}\\\"\".format(sentence=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad80c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sem_prompt(sentence):\n",
    "    return (\"The word 'some' can be understood in several ways. \" \n",
    "            \"One way is to understand it as some and possibly all. \" \n",
    "            \"Thus, a sentence like 'Some daffodils are flowers' \" \n",
    "            \"should be considered true, even though all daffodils are flowers. \" \n",
    "            \"Now evaluate whether the following sentence in the \"\n",
    "            \"double quotation marks is true or false. \"\n",
    "            \"Respond with only one word, either true or false. \"\n",
    "            \"Do not include any new lines or empty spaces. \"\n",
    "            \"\\\"{sentence}\\\"\".format(sentence=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4ffb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prag_prompt(sentence):\n",
    "    return (\"The word 'some' can be understood in several ways. \" \n",
    "            \"One way is to understand it as some but not all. \" \n",
    "            \"Thus, a sentence like 'Some daffodils are flowers' \" \n",
    "            \"should be considered false because, in fact, all daffodils are flowers. \" \n",
    "            \"Now evaluate whether the following sentence in the \"\n",
    "            \"double quotation marks is true or false. \"\n",
    "            \"Respond with only one word, either true or false. \"\n",
    "            \"Do not include any new lines or empty spaces. \"\n",
    "            \"\\\"{sentence}\\\"\".format(sentence=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dfd5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The word \\'some\\' can be understood in several ways. One way is to understand it as some but not all. Thus, a sentence like \\'Some daffodils are flowers\\' should be considered false because, in fact, all daffodils are flowers. Now evaluate whether the following sentence in the double quotation marks is true or false. Respond with only one word, either true or false. Do not include any new lines or empty spaces. \"All lobsters are fish.\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prag_prompt('All lobsters are fish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd5f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_gpt4(item):\n",
    "    \"\"\"\n",
    "    Generate sentence continuation with GPT-4. \n",
    "    Print out the output in a dictionary format.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = openai.ChatCompletion.create(\n",
    "        model = \"gpt-4-0613\",\n",
    "        messages = [{\"role\": \"user\", \"content\":item}],\n",
    "        max_tokens=30,\n",
    "        temperature=0, #argmax\n",
    "        n= 1,\n",
    "        stop= [\".\",\"\\n\"]\n",
    "        )\n",
    "    \n",
    "    output_dict = output.to_dict()['choices'][0].to_dict()\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596e6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_gpt3(item):\n",
    "    \"\"\"\n",
    "    Generate sentence continuation with GPT-3. \n",
    "    Print out the output in a dictionary format.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = openai.Completion.create(\n",
    "        model = \"text-davinci-003\",\n",
    "        prompt = item,\n",
    "        suffix = \".\",\n",
    "        max_tokens=30, \n",
    "        temperature=0, # argmax\n",
    "        n= 1,\n",
    "        logprobs= 1,\n",
    "        #stop= [\".\"]\n",
    "        )\n",
    "    \n",
    "    output_dict = output.to_dict()['choices'][0].to_dict()\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46b799a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserted_prob_gpt3(experiment_item):\n",
    "    \"\"\"\n",
    "    Generate sentence continuation with GPT-3. \n",
    "    Print out the log probability for the inserted tokens in a dictionary format.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = openai.Completion.create(\n",
    "        model = \"text-davinci-003\",\n",
    "        prompt = experiment_item,\n",
    "        suffix = \".\",\n",
    "        max_tokens = 30,\n",
    "        temperature = 0,\n",
    "        n = 1,\n",
    "        #stream = False,\n",
    "        logprobs = 1,\n",
    "        stop = [\".\"]\n",
    "        )\n",
    "    \n",
    "    toplogprob = output.to_dict()['choices'][0].to_dict()['logprobs'].to_dict()[\"top_logprobs\"]\n",
    "    # We are using the [\"top_logprobs\"] because it notes each output token and its logprob together in a JSON entry.\n",
    "    # Each [\"top_logprobs\"] is converted into a Python dictionary: output token as a key and logprob as a value. \n",
    "    \n",
    "    response = []\n",
    "    \n",
    "    for i in range(0,len(toplogprob)):\n",
    "        if 'true' in toplogprob[i] or 'True' in toplogprob[i] or 'False' in toplogprob[i] or 'false'.casefold() in toplogprob[i]:\n",
    "            response.append(toplogprob[i].to_dict())\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f05b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt4(filename):\n",
    "    data_path = \"{FILE}.csv\".format(FILE=filename)\n",
    "    df_item = pd.read_csv(data_path)\n",
    "    \n",
    "    logprob = []\n",
    "    base_res = []\n",
    "    sem_res = []\n",
    "    prag_res = []\n",
    "    t_type = []\n",
    "    prompt_type = []\n",
    "    sentence = []\n",
    "    \n",
    "    with tqdm(total=df_item.shape[0]) as pbar:\n",
    "        for index, row in df_item.iterrows():\n",
    "            \n",
    "            t = row['type']\n",
    "            t_type.append(t_type)\n",
    "        \n",
    "            b = base_prompt(row['sentence'])\n",
    "            baseline = insert_gpt4(b)\n",
    "            base_res.append(baseline['message']['content'])            \n",
    "            \n",
    "            s = sem_prompt(row['sentence'])\n",
    "            semantic = insert_gpt4(s)\n",
    "            sem_res.append(semantic['message']['content'])\n",
    "            \n",
    "            p = prag_prompt(row['sentence'])\n",
    "            pragmatic = insert_gpt4(p)\n",
    "            prag_res.append(pragmatic['message']['content'])\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "    df_item['base_res'] = base_res\n",
    "    df_item['sem_res'] = sem_res\n",
    "    df_item['prag_res'] = prag_res\n",
    "    \n",
    "    df_item.to_csv(\"{TASK}_gpt4_output.csv\".format(TASK=filename))\n",
    "    \n",
    "    return df_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ebcfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gpt3(filename):\n",
    "    data_path = \"{FILE}.csv\".format(FILE=filename)\n",
    "    df_item = pd.read_csv(data_path)\n",
    "    \n",
    "    logprob = []\n",
    "    base_res = []\n",
    "    base_prob = []\n",
    "    sem_res = []\n",
    "    sem_prob = []\n",
    "    prag_res = []\n",
    "    prag_prob = []\n",
    "    t_type = []\n",
    "    prompt_type = []\n",
    "    sentence = []\n",
    "    \n",
    "    with tqdm(total=df_item.shape[0]) as pbar:\n",
    "        for index, row in df_item.iterrows():\n",
    "            \n",
    "            t = row['type']\n",
    "            t_type.append(t_type)\n",
    "        \n",
    "            b = base_prompt(row['sentence'])\n",
    "            baseline = insert_gpt3(b)\n",
    "            baseline_prob = inserted_prob_gpt3(b)\n",
    "            \n",
    "            base_res.append(baseline['text'].strip()) \n",
    "            base_prob.append(baseline_prob[0].values())\n",
    "            \n",
    "            s = sem_prompt(row['sentence'])\n",
    "            semantic = insert_gpt3(s)\n",
    "            semantic_prob = inserted_prob_gpt3(s)\n",
    "            \n",
    "            sem_res.append(semantic['text'].strip())\n",
    "            sem_prob.append(semantic_prob[0].values())\n",
    "            \n",
    "            p = prag_prompt(row['sentence'])\n",
    "            pragmatic = insert_gpt3(p)\n",
    "            pragmatic_prob = inserted_prob_gpt3(p)\n",
    "            \n",
    "            prag_res.append(pragmatic['text'].strip())\n",
    "            prag_prob.append(pragmatic_prob[0].values())\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "    df_item['base_res'] = base_res\n",
    "    df_item['base_prob'] = base_prob\n",
    "    df_item['sem_res'] = sem_res\n",
    "    df_item['sem_prob'] = sem_prob\n",
    "    df_item['prag_res'] = prag_res\n",
    "    df_item['prag_prob'] = prag_prob\n",
    "    \n",
    "    df_item.to_csv(\"{TASK}_gpt3_output.csv\".format(TASK=filename))\n",
    "    \n",
    "    return df_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123c654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
